<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reporte de Destilación</title>
    <style>
        :root {
            --bg-color: #0d1117;
            --card-bg: #161b22;
            --text-color: #c9d1d9;
            --accent-color: #58a6ff;
            --code-bg: #21262d;
            --border-color: #30363d;
        }
        body {
            font-family: 'Segoe UI', 'Roboto', Helvetica, Arial, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        h1 {
            color: var(--accent-color);
            text-align: center;
            font-size: 2.5rem;
            margin-bottom: 40px;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 20px;
        }
        .section-card {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 30px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        h2 {
            color: var(--accent-color);
            margin-top: 0;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        p { margin-bottom: 16px; }
        
        .code-block {
            background-color: var(--code-bg);
            padding: 12px;
            border-radius: 6px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            border: 1px solid var(--border-color);
            color: #ff7b72;
            margin: 10px 0;
        }
        
        .image-gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 24px;
        }
        .img-wrapper {
            position: relative;
            overflow: hidden;
            border-radius: 6px;
            border: 1px solid var(--border-color);
            cursor: pointer;
        }
        img {
            width: 100%;
            height: auto;
            display: block;
            transition: transform 0.3s ease;
        }
        .img-wrapper:hover img {
            transform: scale(1.03);
        }
        
        /* Lightbox */
        #lightbox {
            display: none;
            position: fixed;
            z-index: 1000;
            top: 0; left: 0;
            width: 100%; height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }
        #lightbox img {
            max-width: 90%;
            max-height: 90%;
            border: 2px solid var(--accent-color);
            border-radius: 4px;
        }
        #lightbox.active { display: flex; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Reporte: Destilación de Conocimiento</h1>
        <div class="section-card">
            <h2>Reporte de Práctica</h2>
            <p>Reporte Técnico: Destilación de Conocimiento</p>
            <p>Implementación de BERT-Tiny asistido por DistilBERT en Hardware de Nueva</p>
            <p>Generación</p>
            <p>Equipo de Desarrollo de IA</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-22 194159.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>25 de enero de 2026</h2>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-22 194323.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>Resumen</h2>
            <p>Este documento detalla el proceso de preparación, entrenamiento y validación de un</p>
            <p>modelo de lenguaje mediante la técnica de Knowledge Distillation. El proyecto abordó de-</p>
            <p>safíos técnicos significativos relacionados con la compatibilidad de hardware de última</p>
            <p>generación (NVIDIA RTX 5060 Ti) y la optimización de entornos de ejecución en Windows</p>
            <p>11.</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-22 195935.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>1 Fase 1: Preparación del Sistema</h2>
            <p>Debido a la incompatibilidad inicial de Python 3.13 con las librerías de Deep Learning ac-</p>
            <p>tuales, se realizó un downgrade controlado a Python 3.11.9.</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-22 200514.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>1.1 Configuración del Entorno</h2>
            <p>Se procedió a la creación de una estructura de proyecto aislada mediante entornos virtuales</p>
            <p>de Python para garantizar la reproducibilidad.</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-22 202354.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>1 # Creacion del proyecto</h2>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-23 194636.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>2 mkdir PracticaDestilacion</h2>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Imagenes/Captura de pantalla 2026-01-23 202417.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>3 cd PracticaDestilacion</h2>
            <p>4</p>
        </div>
        <div class="section-card">
            <h2>5 # Entorno virtual</h2>
        </div>
        <div class="section-card">
            <h2>6 python -m venv venv</h2>
        </div>
        <div class="section-card">
            <h2>7 .\venv\Scripts\activate</h2>
            <p>8</p>
        </div>
        <div class="section-card">
            <h2>9 # Instalacion de dependencias (Nightly para soporte serie 50)</h2>
        </div>
        <div class="section-card">
            <h2>10 pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/</h2>
            <p>cu124</p>
        </div>
        <div class="section-card">
            <h2>11 pip install transformers datasets scikit-learn accelerate</h2>
            <p>Listing 1: Comandos de preparación en PowerShell</p>
        </div>
        <div class="section-card">
            <h2>2 Fase 2: Script de Entrenamiento (destilacion_gpu.py)</h2>
            <p>Dada la arquitectura sm_120 de la RTX 5060 Ti (Serie 50), se optó por una estrategia de ejecu-</p>
            <p>ción en CPU para asegurar la estabilidad del proceso, evitando errores de falta de imágenes</p>
            <p>de kernel en PyTorch.</p>
            <p>1</p>
        </div>
        <div class="section-card">
            <h2>2.1 Lógica de Destilación</h2>
            <p>Se implementó un Trainer personalizado para calcular la pérdida de destilación utilizando</p>
            <p>la Divergencia de Kullback-Leibler (KL Divergence), balanceando la pérdida del estudiante</p>
            <p>con las predicciones del profesor .</p>
            <p>Configuración de Modelos</p>
            <p>Profesor: distilbert-base-uncased-finetuned-sst-2-english</p>
            <p>Estudiante: prajjwal1/bert-tiny</p>
        </div>
        <div class="section-card">
            <h2>3 Fase 3: Validación y Comparativa</h2>
            <p>Tras un entrenamiento de aproximadamente 2 horas en CPU (12,630 pasos), se ejecutó un</p>
            <p>script de validación para comparar el rendimiento real del modelo destilado frente al pro-</p>
            <p>fesor .</p>
        </div>
        <div class="section-card">
            <h2>3.1 Resultados Métricos</h2>
            <p>• Precisión Final (Accuracy): 82.45%</p>
            <p>• Parámetros del Profesor: 67M</p>
            <p>• Parámetros del Estudiante: 4.4M</p>
            <p>• Factor de Reducción: 15.3x más pequeño.</p>
        </div>
        <div class="section-card">
            <h2>4 Registro de Errores y Soluciones</h2>
            <p>A continuación se detallan los conflictos técnicos encontrados durante la práctica y las so-</p>
            <p>luciones aplicadas:</p>
            <p># Error Detectado Solución Aplicada</p>
        </div>
        <div class="section-card">
            <h2>1 TypeError en evalua-</h2>
            <p>tion_strategy</p>
            <p>Actualización de sintaxis a</p>
            <p>eval_strategy para Transfor-</p>
            <p>mers v4.4x.</p>
        </div>
        <div class="section-card">
            <h2>2 Incompatibilidad CUDA</h2>
            <p>sm_120</p>
            <p>Desactivación forza-</p>
            <p>da de GPU mediante</p>
            <p>CUDA_VISIBLE_DEVICES="".</p>
        </div>
        <div class="section-card">
            <h2>3 Conflicto token_type_ids Filtrado manual de entradas para el</h2>
            <p>modelo DistilBERT dentro del com-</p>
            <p>pute_loss.</p>
        </div>
        <div class="section-card">
            <h2>4 Error de carga de tensores</h2>
            <p>mixtos</p>
            <p>Forzado explícito de device="cpu"</p>
            <p>en todos los componentes del mode-</p>
            <p>lo.</p>
            <p>Cuadro 1: Bitácora de Troubleshooting.</p>
            <p>2</p>
        </div>
        <div class="section-card">
            <h2>5 Conclusiones</h2>
            <p>La práctica demuestra que es posible obtener un modelo altamente eficiente (BERT-Tiny)</p>
            <p>con una pérdida de precisión aceptable respecto a un modelo mayor . A pesar de las limitacio-</p>
            <p>nes temporales del hardware Serie 50, la ejecución en CPU permitió validar la arquitectura</p>
            <p>de destilación de forma exitosa.</p>
            <p>3</p>
        </div>
    </div>
    
    <div id="lightbox" onclick="this.classList.remove('active')">
        <img id="lightbox-img" src="" alt="Full size">
    </div>

    <script>
        function openLightbox(element) {
            const imgSrc = element.querySelector('img').src;
            document.getElementById('lightbox-img').src = imgSrc;
            document.getElementById('lightbox').classList.add('active');
        }
    </script>
</body>
</html>