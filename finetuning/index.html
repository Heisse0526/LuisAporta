<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Manual Finetuning (Apple Silicon)</title>
    <style>
        :root {
            --bg-color: #0d1117;
            --card-bg: #161b22;
            --text-color: #c9d1d9;
            --accent-color: #58a6ff;
            --code-bg: #21262d;
            --border-color: #30363d;
        }
        body {
            font-family: 'Segoe UI', 'Roboto', Helvetica, Arial, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        h1 {
            color: var(--accent-color);
            text-align: center;
            font-size: 2.5rem;
            margin-bottom: 40px;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 20px;
        }
        .section-card {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 30px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        h2 {
            color: var(--accent-color);
            margin-top: 0;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        p { margin-bottom: 16px; }
        
        .code-block {
            background-color: var(--code-bg);
            padding: 12px;
            border-radius: 6px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            border: 1px solid var(--border-color);
            color: #ff7b72;
            margin: 10px 0;
        }
        
        .image-gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 24px;
        }
        .img-wrapper {
            position: relative;
            overflow: hidden;
            border-radius: 6px;
            border: 1px solid var(--border-color);
            cursor: pointer;
        }
        img {
            width: 100%;
            height: auto;
            display: block;
            transition: transform 0.3s ease;
        }
        .img-wrapper:hover img {
            transform: scale(1.03);
        }
        
        /* Lightbox */
        #lightbox {
            display: none;
            position: fixed;
            z-index: 1000;
            top: 0; left: 0;
            width: 100%; height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }
        #lightbox img {
            max-width: 90%;
            max-height: 90%;
            border: 2px solid var(--accent-color);
            border-radius: 4px;
        }
        #lightbox.active { display: flex; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Manual: Fine-Tuning con LoRA</h1>
        <div class="section-card">
            <h2>Introducción</h2>
            <p>Memoria de Práctica: Fine-Tuning de LLMs con LoRA en Apple Silicon</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 3.54.06.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.04.09.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>1. Introducción y Objetivos</h2>
            <p>El objetivo de esta práctica es realizar un proceso de Fine-Tuning (ajuste fino) sobre un Modelo de Lenguaje Grande (LLM) pre-entrenado. Según la documentación teórica de la asignatura, buscamos especializar un modelo generalista para convertirlo en un asistente experto en formalidad, capaz de responder siempre utilizando el tratamiento de "usted" y manteniendo un tono profesional.</p>
            <p>Para esta implementación, hemos adaptado el flujo de trabajo estándar (generalmente diseñado para GPUs NVIDIA y Linux) para ejecutarlo nativamente en un MacBook Pro con chip M1/M2/M3 (Apple Silicon), utilizando la librería MLX de Apple, que permite un uso eficiente de la memoria unificada.</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.17.03.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.18.04.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>2. Preparación del Entorno en Mac (Apple Silicon)</h2>
            <p>A diferencia del entorno con CUDA (NVIDIA), en Mac necesitamos preparar un entorno aislado que utilice la potencia de la GPU de Apple (Metal).</p>
            <p>Pasos Realizados:</p>
            <p>1. Creación del directorio de trabajo: Lo primero es generar una estructura limpia para evitar conflictos con otros proyectos.</p>
            <div class="code-block">cd /Users/ivanzsasz/2DAM/ASI/FineTuningASI</div>
            <p>2. Aislamiento del entorno (Virtual Environment): Creamos un entorno virtual (venv) para instalar las librerías específicas de esta práctica sin contaminar la instalación global de Python en el Mac.</p>
            <div class="code-block">python3 -m venv venv</div>
            <div class="code-block">source venv/bin/activate</div>
            <p>3. Gestión eficiente del almacenamiento: Los LLMs son pesados (varios GB). Configuramos la variable HF_HOME para forzar a que los modelos descargados de Hugging Face se guarden dentro de nuestra carpeta de proyecto. Esto facilita la limpieza posterior: al borrar la carpeta del proyecto, se borra también la caché de modelos.</p>
            <div class="code-block">export HF_HOME=$(pwd)/cache</div>
            <p>4. Instalación de librerías nativas: Instalamos mlx-lm, la librería optimizada por Apple para el entrenamiento de modelos de lenguaje en sus chips, junto con las herramientas de Hugging Face.</p>
            <div class="code-block">pip install mlx-lm huggingface_hub</div>
            <p>Evidencia del proceso: Aquí se muestra la instalación de las dependencias y la descarga de los paquetes necesarios.</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.21.17.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.22.54.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>3. Creación y Estructuración del Dataset</h2>
            <p>La calidad del dataset es el factor más crítico ("Garbage in, garbage out"). Hemos creado un archivo JSONL (JSON Lines) siguiendo el formato de instrucción de Mistral (<s>[INST] ... [/INST] ... </s>).</p>
            <p>Contenido del archivo train.jsonl: Hemos definido pares de instrucción y respuesta ideal. El modelo aprenderá que ante preguntas comunes, debe responder con una estructura gramatical formal y educada.</p>
            <p>Ejemplo de entrada:</p>
            <div class="code-block">{"text": "<s>[INST] Hola, ¿quién eres? [/INST] Buenas tardes. Soy un asistente virtual diseñado para ayudarle. ¿En qué puedo servirle hoy? </s>"}</div>
            <p>Comandos utilizados:</p>
            <div class="code-block">nano datos_asistente_formal.jsonl</div>
            <p># (Tras guardar el contenido)</p>
            <div class="code-block">mv datos_asistente_formal.jsonl train.jsonl</div>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.23.10.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.24.51.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>4. Entrenamiento del Modelo (Fine-Tuning con LoRA)</h2>
            <p>En lugar de reentrenar todo el modelo (Full Fine-Tuning), que requeriría recursos industriales, utilizamos LoRA (Low-Rank Adaptation). Esta técnica congela el modelo base y solo entrena capas pequeñas de adaptadores, siendo mucho más rápido y eficiente.</p>
            <p>Primer intento (Error de configuración):</p>
            <p>Inicialmente, intentamos usar el argumento --lora-layers, pero la versión actualizada de mlx ha cambiado la nomenclatura.</p>
            <p>Segundo intento (Error de Dataset):</p>
            <p>Al corregir el comando, surgió un error indicando que no encontraba el set de entrenamiento. Esto se debió a que la herramienta busca por defecto archivos con nombres estándar (train.jsonl, valid.jsonl). Solución: Renombramos nuestro archivo y creamos copias para validación y test, satisfaciendo los requisitos de la librería.</p>
            <div class="code-block">cp train.jsonl valid.jsonl</div>
            <div class="code-block">cp train.jsonl test.jsonl</div>
            <p>Ejecución Exitosa:</p>
            <p>Finalmente, lanzamos el entrenamiento con los parámetros ajustados para el hardware local:</p>
            <p>Model: mistralai/Mistral-7B-v0.1</p>
            <p>Iteraciones: 100 (Suficientes para una prueba de concepto).</p>
            <p>Batch size: 1 (Para minimizar el uso de VRAM).</p>
            <p>Capas (LoRA): 4.</p>
            <div class="code-block">python -m mlx_lm.lora \</div>
            <p>--model mistralai/Mistral-7B-v0.1 \</p>
            <p>--train \</p>
            <p>--data ./ \</p>
            <p>--iters 100 \</p>
            <p>--batch-size 1 \</p>
            <p>--num-layers 4 \</p>
            <p>--adapter-path adapters</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.26.17.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.29.58.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>5. Fusión y Conversión del Modelo (GGUF)</h2>
            <p>Una vez obtenidos los "adaptadores" (los archivos ligeros con el aprendizaje nuevo), debemos fusionarlos con el cerebro original (Mistral) y convertir el resultado a un formato ejecutable estándar (GGUF) para usarlo en aplicaciones como LM Studio.</p>
            <p>A. Fusión (Fuse)</p>
            <p>Unimos los pesos base con nuestros adaptadores LoRA:</p>
            <div class="code-block">python -m mlx_lm.fuse \</div>
            <p>--model mistralai/Mistral-7B-v0.1 \</p>
            <p>--adapter-path adapters \</p>
            <p>--save-path modelo_fusionado</p>
            <p>B. Preparación de llama.cpp</p>
            <p>Clonamos la herramienta estándar de conversión. Tuvimos que resolver dependencias faltantes manualmente, ya que el archivo requirements.txt pedía versiones incompatibles con nuestra versión de Python. Solución: Instalación manual de paquetes esenciales (numpy, sentencepiece, gguf, protobuf, torch).</p>
            <p>C. Conversión a GGUF</p>
            <p>Aquí encontramos un obstáculo técnico importante. El PDF original sugería una cuantización directa a q4_k_m. Sin embargo, el script de conversión actual no soporta esa cuantización "al vuelo". Solución Técnica: Optamos por convertir el modelo a precisión media (f16). Aunque el archivo resultante es más pesado (aprox. 14GB vs 4GB), conserva una mayor calidad y precisión en las respuestas, lo cual es ideal para demostraciones.</p>
            <p>Comando final exitoso:</p>
            <div class="code-block">python llama.cpp/convert_hf_to_gguf.py modelo_fusionado \</div>
            <p>--outfile modelo-formal.gguf \</p>
            <p>--outtype f16</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.30.12.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.31.03.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>6. Pruebas y Resultados</h2>
            <p>El archivo final modelo-formal.gguf se importó en LM Studio.</p>
            <p>Prueba de Inferencia: Al preguntar "Hola, ¿quién eres?", el modelo respondió con la frase exacta entrenada: "Buenas tardes. Soy un asistente virtual diseñado para ayudarle...".</p>
            <p>Esto confirma que los pesos del modelo fueron modificados exitosamente mediante LoRA, logrando que el LLM abandone su comportamiento genérico y adopte la personalidad definida en el dataset.</p>
            <div class="image-gallery">
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.33.49.png" alt="Captura" loading="lazy">
                </div>
                <div class="img-wrapper" onclick="openLightbox(this)">
                    <img src="Copia de Captura de pantalla 2026-02-05 a las 4.34.08.png" alt="Captura" loading="lazy">
                </div>
            </div>
        </div>
        <div class="section-card">
            <h2>7. Conclusiones y Aprendizaje</h2>
            <p>La realización de esta práctica ha permitido consolidar varios conceptos clave del Fine-Tuning:</p>
            <p>Viabilidad en Apple Silicon: Se ha demostrado que es posible realizar entrenamientos de IA complejos en hardware de consumo (MacBook Pro) usando librerías optimizadas como MLX, sin necesidad de servidores con GPUs NVIDIA industriales.</p>
            <p>Importancia del Dataset: Los errores encontrados durante el proceso (como la falta de archivos de validación) refuerzan la teoría de que la preparación y estructuración de los datos es la parte más sensible del proceso.</p>
            <p>Gestión de Dependencias: La resolución de conflictos de versiones (pip, torch, matplotlib) es una habilidad esencial en la ingeniería de IA, ya que las herramientas evolucionan muy rápido.</p>
            <p>Flexibilidad de LoRA: Hemos comprobado cómo LoRA permite modificar el comportamiento de un modelo gigante (7B de parámetros) entrenando solo una fracción mínima de parámetros, haciendo el proceso rápido y eficiente.</p>
        </div>
    </div>
    
    <div id="lightbox" onclick="this.classList.remove('active')">
        <img id="lightbox-img" src="" alt="Full size">
    </div>

    <script>
        function openLightbox(element) {
            const imgSrc = element.querySelector('img').src;
            document.getElementById('lightbox-img').src = imgSrc;
            document.getElementById('lightbox').classList.add('active');
        }
    </script>
</body>
</html>